{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bd8de7-ebbd-482b-8d08-e88a3a0c096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/23 10:53:36 WARN Utils: Your hostname, victor-Lenovo-ideapad-330-15IKB resolves to a loopback address: 127.0.1.1; using 192.168.1.74 instead (on interface wlp2s0)\n",
      "24/04/23 10:53:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/23 10:53:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.74:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Exemplo Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x72abe0311910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Definir o caminho do Spark corretamente\n",
    "os.environ['SPARK_HOME'] = '/home/daiane/spark-3.5.1-bin-hadoop3/'\n",
    "\n",
    "# Definir o caminho do Java corretamente\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-1.17.0-openjdk-amd64/'\n",
    "\n",
    "# Iniciar uma sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Exemplo Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Testar a sessão Spark\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a6e88-b3b9-4b31-a746-c0f142154ae9",
   "metadata": {},
   "source": [
    "\n",
    "Este projeto prático foca na transformação e integração de dados usando PySpark. <br>\n",
    "Vou trabalhar com dois conjuntos de dados, realizar várias transformações como adicionar colunas, renomear colunas, eliminar colunas<br>\n",
    "desnecessárias, unir dataframes e, por fim, escrever os resultados tanto em um armazém Hive quanto em um sistema de arquivos HDFS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19dcaa88-37ee-40eb-9e67-f24196647533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyspark in /home/daiane/anaconda3/lib/python3.11/site-packages (3.5.1)\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/daiane/anaconda3/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=0091535511e2ccaff1432695ddb07ab91ba643ec54d8452292aa6f4a76131f15\n",
      "  Stored in directory: /home/daiane/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, findspark\n",
      "Successfully installed findspark-2.0.1 wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget pyspark  findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d989081-4c72-4f86-8135-520ed1706f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6300a0ae-747f-43c0-9718-00d8794527fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d9177d5-e2d3-4a62-8c66-897f638dd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark context \n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbadaa25-3e85-4cf6-aaf5-c2a83d3132f4",
   "metadata": {},
   "source": [
    "### Task 1: Carregar datasets em DataFrames do PySpark. \n",
    "1. https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv  \n",
    "2. https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b0f7c62-f9c8-49f7-9722-f4a0d1c0c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................] 2688 / 2688"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dataset2 (1).csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "link_1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n",
    "link_2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n",
    "wget.download(link_1)\n",
    "wget.download(link_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28852ae7-c297-40ef-8ce1-3b7ff31fd754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc858b-a6c4-456b-abe0-055f7de9a6ce",
   "metadata": {},
   "source": [
    "### Task 2: Exibir o esquema de ambos os dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3c4e2d9-4780-44c4-aa87-61d247527863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date_column: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      " |-- notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5966a19-6ed2-488a-8752-0154d7442c2c",
   "metadata": {},
   "source": [
    "### Task3: Adicionar uma nova coluna a cada dataframe, ano para df1 e trimestre para df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2a99c3f-fcf2-4709-b159-87ba5d692079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, quarter, to_date\n",
    "df1 = df1.withColumn('year', year(to_date('date_column', 'dd/MM/yyyy')))\n",
    "df2 = df2.withColumn('quarter', quarter(to_date('transaction_date','dd/MM/yyyy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da4eed-ce1e-4255-8659-db168744f130",
   "metadata": {},
   "source": [
    "### Task 4: Renomear a coluna \"amount\" para para \"transaction\" em df1 e a coluna \"value\" para \"transaction_value\" em df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "081cd625-c656-438e-b4f2-57cfe66574e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n",
    "df2 = df2.withColumnRenamed('value', 'transaction_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ed54b-084e-4a0b-90f7-62ff08957c5d",
   "metadata": {},
   "source": [
    "### Task 5: Exclua as colunas desnecessárias \"description\" e \"location\" de df1 e \"notes\" de df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8eaa9ae-8afc-4e8d-9a2b-734f7538bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop('description', 'location')\n",
    "df2 = df2.drop('notes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8935649-fd88-48d4-8e5a-e78e21fca3c0",
   "metadata": {},
   "source": [
    "### Task 6: Junte df1 e df2 com base na coluna comum customer_id e crie um novo dataframe chamado joined_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85d90ba4-d0c6-4e3a-80d2-489d0a279a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df1.join(df2, 'customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25007fb0-f96c-48cf-9c28-097a7152d126",
   "metadata": {},
   "source": [
    "### Task 7: Filtrar joined_df para incluir apenas transações onde \"transaction_amount\" é maior que 1000 e criar um novo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17cfc06e-ea75-4be1-82ad-4f7d97079cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = joined_df.filter(\"transaction_amount > 1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253730f9-4946-46ce-b560-d928b7dc7019",
   "metadata": {},
   "source": [
    "### Task 8: Agregação de dados por cliente, calcular o valor total das transações para cada cliente em filtered_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "240469d8-3f40-4be2-a98e-52bede0fb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "total_amount_per_customer = filtered_df.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35bd0000-5f63-4dfd-b9f9-124d98cf1422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|total_amount|\n",
      "+-----------+------------+\n",
      "|         31|        3200|\n",
      "|         85|        1800|\n",
      "|         78|        1500|\n",
      "|         34|        1200|\n",
      "|         81|        5500|\n",
      "|         28|        2600|\n",
      "|         76|        2600|\n",
      "|         27|        4200|\n",
      "|         91|        3200|\n",
      "|         22|        1200|\n",
      "|         93|        5500|\n",
      "|          1|        5000|\n",
      "|         52|        2600|\n",
      "|         13|        4800|\n",
      "|          6|        4500|\n",
      "|         16|        2600|\n",
      "|         40|        2600|\n",
      "|         94|        1200|\n",
      "|         57|        5500|\n",
      "|         54|        1500|\n",
      "+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_amount_per_customer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefd274-2510-4e21-b484-1145c8581ff8",
   "metadata": {},
   "source": [
    "### Task 9:Escrever \"total_amount_per_customer\" em uma tabela Hive chamada \"customer_totals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3c6aa8c-efaf-47bb-bf91-85bb2dd2a9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_amount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cadee7-f23f-4466-92ee-814e0cf615a9",
   "metadata": {},
   "source": [
    "### Task 10: Escrever \"filtered_df\" no HDFS no formato parquet em um arquivo chamado filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4ca42cb-b83e-4f82-a264-5ac1ca5826d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "filtered_df.write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d36d7-be59-4db2-9743-52cb85602c7b",
   "metadata": {},
   "source": [
    "### Task 11: Adicionar uma nova coluna chamada high_value ao df1, indicando se o transaction_amount é maior que 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0739a407-275c-49ea-8440-829ecbd4dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "df1 = df1.withColumn('high_value', when(df1.transaction_amount > 5000, lit(\"Yes\")).otherwise(lit(\"No\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82bc6d0-e2b2-4ac5-8fa8-4bafcaed230e",
   "metadata": {},
   "source": [
    "### Task 12: cacular o valor médio da transação para cada trimestre no df2 e criar um novo dataframe chamado average_value_per_quarter com a coluna avg_trans_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3cdc9c33-a56f-4da1-8fc8-f1a0cae6e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "average_value_per_quarter = df2.groupBy('quarter').agg(avg(\"transaction_value\").alias(\"avg_trans_val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "674f2ddd-4ff5-453e-8fed-73a7e26437de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|quarter|     avg_trans_val|\n",
      "+-------+------------------+\n",
      "|      1| 1111.111111111111|\n",
      "|      3|1958.3333333333333|\n",
      "|      4| 816.6666666666666|\n",
      "|      2|            1072.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_value_per_quarter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c70aef-b078-49e7-bf61-f85091f8640a",
   "metadata": {},
   "source": [
    "# Task 13: Salvar o resultado em uma tabela Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d38b7b4-13a6-4c7a-8654-8a7f74f20821",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91204468-0cd1-4e60-8f73-a90044c25bf5",
   "metadata": {},
   "source": [
    "### Task 14: Calcular o valor de tansação por ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd644553-b986-45aa-8fe8-f651ed7f60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value_per_year = df1.groupBy('year').agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f87d6a5b-d0da-4b0c-8050-2d7f5c14b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|year|total_transaction_val|\n",
      "+----+---------------------+\n",
      "|2025|                25700|\n",
      "|2027|                25700|\n",
      "|2023|                28100|\n",
      "|2022|                29800|\n",
      "|2026|                25700|\n",
      "|2029|                25700|\n",
      "|2030|                 9500|\n",
      "|2028|                25700|\n",
      "|2024|                25700|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_value_per_year.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e59f19-87e7-4e3f-bfc7-4c177626b5ee",
   "metadata": {},
   "source": [
    "### Escrever o total por ano no HDFS em um CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "44a3c75c-3593-432d-8b12-5bc780abcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ab10a-f5ca-422a-bb93-af7d8d89cb07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
